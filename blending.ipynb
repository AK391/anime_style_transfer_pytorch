{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stylegan2-ada-pytorch/blending.ipynb\n",
    "\n",
    "from training.networks import Generator\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_params(G: Generator) -> dict:\n",
    "    params = dict(\n",
    "        [(res, {}) for res in G.synthesis.block_resolutions] + [(\"mapping\", {})]\n",
    "    )\n",
    "    # G params: mapping.xxx / synthesys.b128.xxx\n",
    "    for n, p in sorted(list(G.named_buffers()) + list(G.named_parameters())):\n",
    "        if n.startswith(\"mapping\"):\n",
    "            params[\"mapping\"][n] = p\n",
    "        else:\n",
    "            res = int(n.split(\".\")[1][1:])\n",
    "            params[res][n] = p\n",
    "    return params\n",
    "\n",
    "\n",
    "def blend_models(G_low: Generator, G_high: Generator, swap_layer: int, blend_width: float = 3) -> Generator:\n",
    "    params_low = gather_params(G_low)\n",
    "    params_high = gather_params(G_high)\n",
    "\n",
    "    for layer_idx, res in enumerate(G_low.synthesis.block_resolutions):\n",
    "        x = layer_idx - swap_layer\n",
    "        \n",
    "        if blend_width is not None:\n",
    "            assert blend_width > 0\n",
    "            exponent = - x / blend_width\n",
    "            y = 1 / (1 + math.exp(exponent))\n",
    "        else:\n",
    "            y = 1 if x > 0 else 0\n",
    "            \n",
    "        for n, p in params_high[res].items():\n",
    "            params_high[res][n] = params_high[res][n] * y + params_low[res][n] * (1 - y)\n",
    "\n",
    "    state_dict = {}\n",
    "    for _, p in params_high.items():\n",
    "        state_dict.update(p)\n",
    "\n",
    "    G_mix = deepcopy(G_high)\n",
    "    G_mix.load_state_dict(state_dict)\n",
    "    return G_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffhq_path = './ffhq_512.pkl'\n",
    "# met_path = './metfaces.pkl'\n",
    "met_path = './training-runs/00024-acanev3-mirror-paper512-batch8-resumeffhq512/network-snapshot-000400.pkl'\n",
    "truncation_psi = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(ffhq_path) as f:\n",
    "    G_low = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(met_path) as f:\n",
    "    G_high = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seed = 2**32 - 1\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    seed = np.random.randint(0, max_seed, dtype=np.int64)\n",
    "    \n",
    "    G_blend = blend_models(G_low, G_high, swap_layer=7, blend_width=3)\n",
    "    all_z = np.stack([np.random.RandomState(seed).randn(G_blend.z_dim)])\n",
    "    all_w = G_blend.mapping(torch.from_numpy(all_z).to(device), None)\n",
    "    w_avg = G_blend.mapping.w_avg\n",
    "    all_w = w_avg + (all_w - w_avg) * truncation_psi\n",
    "\n",
    "    input  = G_low.synthesis(all_w, noise_mode=\"const\")\n",
    "    input2 = G_high.synthesis(all_w, noise_mode=\"const\")\n",
    "    target = G_blend.synthesis(all_w, noise_mode=\"const\")\n",
    "    torchvision.utils.save_image(torch.vstack([input]), f'../../datasets/style_transfer/real/{i}.png', normalize=True)\n",
    "    torchvision.utils.save_image(torch.vstack([target]), f'../../datasets/style_transfer/style/{i}.png', normalize=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2cd1452618544b667d70d0feaebcedea185cfa44139336063f02cf92e1e17174"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch_p38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
